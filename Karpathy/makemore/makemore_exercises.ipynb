{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPNsQM1Q3c4EtdjooIkhU7E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n","\n"],"metadata":{"id":"9G1E9Shj7GgI"}},{"cell_type":"code","source":["#Using counting first, NN approach is implemented for E04\n","\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import random\n","%matplotlib inline\n","words = open('names.txt','r').read().splitlines()"],"metadata":{"id":"A0FSnKOq7IG_","executionInfo":{"status":"ok","timestamp":1736277663999,"user_tz":300,"elapsed":123,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":140,"outputs":[]},{"cell_type":"code","source":["N = torch.zeros((27,27,27), dtype=torch.int32)\n","chars = sorted(list(set(''.join(words))))\n","stoi = {s:i+1 for i,s in enumerate(chars)}\n","stoi['.'] = 0\n","itos = {i:s for s,i in stoi.items()}"],"metadata":{"id":"X8nZDjSH9n9x","executionInfo":{"status":"ok","timestamp":1736277664108,"user_tz":300,"elapsed":1,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":141,"outputs":[]},{"cell_type":"code","source":["for w in words:\n","  chs = ['.']+ list(w) + ['.']\n","  for ch1, ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n","    ix1 = stoi[ch1]\n","    ix2 = stoi[ch2]\n","    ix3 = stoi[ch3]\n","    N[ix1,ix2,ix3] +=1\n",""],"metadata":{"id":"9nwsfCCa9wFj","executionInfo":{"status":"ok","timestamp":1736277668959,"user_tz":300,"elapsed":4851,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":142,"outputs":[]},{"cell_type":"code","source":["P = (N+1).float()\n","P = P / P.sum(1,keepdim=True)"],"metadata":{"id":"EPIWBi5sDeO8","executionInfo":{"status":"ok","timestamp":1736277668960,"user_tz":300,"elapsed":4,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":143,"outputs":[]},{"cell_type":"code","source":["g = torch.Generator()\n"],"metadata":{"id":"E84-BPiOEU7h","executionInfo":{"status":"ok","timestamp":1736277668960,"user_tz":300,"elapsed":3,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":144,"outputs":[]},{"cell_type":"code","source":["for i in range(50):\n","    out = []\n","    ix1, ix2 = 0, 0\n","    while True:\n","        p = N[ix1, ix2].float()\n","        if p.sum() == 0:\n","            p = torch.ones_like(p)\n","        p = p / p.sum()\n","\n","        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n","\n","        out.append(itos[ix3])\n","\n","        if ix3 == 0:\n","            break\n","\n","        ix1, ix2 = ix2, ix3\n","\n","    print(''.join(out))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_HSdZQG1DirU","executionInfo":{"status":"ok","timestamp":1736277668960,"user_tz":300,"elapsed":3,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"66cae6cc-f939-46d3-d9eb-28a37a63cb44"},"execution_count":145,"outputs":[{"output_type":"stream","name":"stdout","text":["cy.\n","intemili.\n","hyn.\n","yea.\n","tuliaayertlierryiah.\n","viele.\n","sta.\n","jayahlor.\n","ulaber.\n","liann.\n","gra.\n","novianib.\n","payukeianey.\n","evyaarsha.\n","morie.\n","xiella.\n","bra.\n","cataelia.\n","gus.\n","na.\n","prannustagus.\n","hon.\n","niya.\n","gwestes.\n","fosefor.\n","gerseon.\n","fiahen.\n","gre.\n","peria.\n","rekshayris.\n","iya.\n","zairo.\n","ovaniyangabenni.\n","ya.\n","pem.\n","den.\n","fane.\n","da.\n","fady.\n","vaya.\n","pren.\n","woluleimukenna.\n","phe.\n","praelliasian.\n","arivany.\n","la.\n","jiollucamr.\n","dalinet.\n","bre.\n","ni.\n"]}]},{"cell_type":"code","source":["nlls = []\n","num_examples = 5\n","\n","for i in range(num_examples):\n","    out = []\n","    ix1, ix2 = 0, 0\n","    while True:\n","        p = N[ix1, ix2].float()\n","        if p.sum() == 0:\n","            p = torch.ones_like(p)\n","        p = p / p.sum()\n","\n","        ix3 = torch.multinomial(p, num_samples=1, replacement=True).item()\n","\n","        out.append(itos[ix3])\n","\n","        if ix3 == 0:\n","            break\n","\n","        prob_correct = p[ix3]\n","        logp = torch.log(prob_correct)\n","        nll = -logp\n","        nlls.append(nll.item())\n","\n","        ix1, ix2 = ix2, ix3\n","\n","    print(f\"Generated sequence {i+1}: {''.join(out)}\")\n","\n","nlls = torch.tensor(nlls)\n","print('=========')\n","print('Average negative log likelihood (loss):', nlls.mean().item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzYSzbhCMLLv","executionInfo":{"status":"ok","timestamp":1736277669066,"user_tz":300,"elapsed":109,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"84aa2e7e-d935-4bf3-8549-e4a3ec42d0fe"},"execution_count":146,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated sequence 1: vignalisammon.\n","Generated sequence 2: deqiya.\n","Generated sequence 3: moran.\n","Generated sequence 4: lid.\n","Generated sequence 5: grayzarce.\n","=========\n","Average negative log likelihood (loss): 2.5011322498321533\n"]}]},{"cell_type":"markdown","source":["E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n"],"metadata":{"id":"HEA5PesnF2j6"}},{"cell_type":"code","source":[],"metadata":{"id":"JJ1Tj55ILZXe","executionInfo":{"status":"ok","timestamp":1736277669066,"user_tz":300,"elapsed":4,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":146,"outputs":[]},{"cell_type":"code","source":["# First, we split the datset randomly\n","\n","random.seed(21)\n","random.shuffle(words)\n","\n","n = len(words)\n","train_split = int(0.8 * n)\n","dev_split = int(0.9 * n)\n","\n","train_words = words[:train_split]\n","dev_words = words[train_split:dev_split]\n","test_words = words[dev_split:]\n","\n","print(f\"Train size: {len(train_words)}, Dev size: {len(dev_words)}, Test size: {len(test_words)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B8x6gthIGE5s","executionInfo":{"status":"ok","timestamp":1736277669066,"user_tz":300,"elapsed":3,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"8601f9ff-263b-4a61-f743-3264354c829e"},"execution_count":147,"outputs":[{"output_type":"stream","name":"stdout","text":["Train size: 25626, Dev size: 3203, Test size: 3204\n"]}]},{"cell_type":"code","source":["# Next, train the bigram\n","\n","bigram_counts = torch.zeros((27, 27), dtype=torch.float32)\n","for word in train_words:\n","    chs = ['.'] + list(word) + ['.']\n","    for ch1, ch2 in zip(chs, chs[1:]):\n","        ix1 = stoi[ch1]\n","        ix2 = stoi[ch2]\n","        bigram_counts[ix1, ix2] += 1\n","\n","bigram_probs = bigram_counts / bigram_counts.sum(dim=1, keepdim=True)\n"],"metadata":{"id":"y8xIiAJRGOgh","executionInfo":{"status":"ok","timestamp":1736277673378,"user_tz":300,"elapsed":4314,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":148,"outputs":[]},{"cell_type":"code","source":["# Next, train the trigram\n","\n","trigram_counts = torch.zeros((27, 27, 27), dtype=torch.float32)\n","for word in train_words:\n","    chs = ['.'] + list(word) + ['.']\n","    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n","        ix1 = stoi[ch1]\n","        ix2 = stoi[ch2]\n","        ix3 = stoi[ch3]\n","        trigram_counts[ix1, ix2, ix3] += 1\n","\n","trigram_probs = trigram_counts / trigram_counts.sum(dim=2, keepdim=True)\n"],"metadata":{"id":"-nSbvLtHGTPU","executionInfo":{"status":"ok","timestamp":1736277676823,"user_tz":300,"elapsed":3446,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":149,"outputs":[]},{"cell_type":"code","source":["# Finally, we evaluate the models with negative log liklihood\n","\n","def compute_loss(words, model_probs, ngram_size):\n","    total_loss = 0\n","    total_chars = 0\n","\n","    for word in words:\n","        chs = ['.'] + list(word) + ['.']\n","        for i in range(len(chs) - ngram_size + 1):\n","            indices = [stoi[chs[j]] for j in range(i, i + ngram_size)]\n","            prob = model_probs[tuple(indices)].item()\n","            if prob > 0:\n","                total_loss += -np.log(prob)\n","            total_chars += 1\n","\n","    return total_loss / total_chars\n","\n","bigram_train_loss = compute_loss(train_words, bigram_probs, 2)\n","bigram_dev_loss = compute_loss(dev_words, bigram_probs, 2)\n","bigram_test_loss = compute_loss(test_words, bigram_probs, 2)\n","\n","trigram_train_loss = compute_loss(train_words, trigram_probs, 3)\n","trigram_dev_loss = compute_loss(dev_words, trigram_probs, 3)\n","trigram_test_loss = compute_loss(test_words, trigram_probs, 3)\n","\n","print(f\"Bigram Model - Train Loss: {bigram_train_loss:.4f}, Dev Loss: {bigram_dev_loss:.4f}, Test Loss: {bigram_test_loss:.4f}\")\n","print(f\"Trigram Model - Train Loss: {trigram_train_loss:.4f}, Dev Loss: {trigram_dev_loss:.4f}, Test Loss: {trigram_test_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uoWr9Bm3GkEM","executionInfo":{"status":"ok","timestamp":1736277681267,"user_tz":300,"elapsed":4445,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"619af937-3d2e-4a27-db00-8b610925ecd1"},"execution_count":150,"outputs":[{"output_type":"stream","name":"stdout","text":["Bigram Model - Train Loss: 2.4542, Dev Loss: 2.4518, Test Loss: 2.4533\n","Trigram Model - Train Loss: 2.0567, Dev Loss: 2.0555, Test Loss: 2.0567\n"]}]},{"cell_type":"markdown","source":["E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n","\n"],"metadata":{"id":"reXzlQdcIETv"}},{"cell_type":"code","source":["def compute_trigram_probs_with_smoothing(counts, alpha):\n","    smoothed_counts = counts + alpha\n","    smoothed_probs = smoothed_counts / smoothed_counts.sum(dim=2, keepdim=True)\n","    return smoothed_probs\n"],"metadata":{"id":"4BC2r-WNICIE","executionInfo":{"status":"ok","timestamp":1736277681267,"user_tz":300,"elapsed":2,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":151,"outputs":[]},{"cell_type":"code","source":["\n","def compute_loss(words, model_probs, ngram_size):\n","    total_loss = 0\n","    total_chars = 0\n","\n","    for word in words:\n","        chs = ['.'] + list(word) + ['.']\n","        for i in range(len(chs) - ngram_size + 1):\n","            indices = [stoi[chs[j]] for j in range(i, i + ngram_size)]\n","            prob = model_probs[tuple(indices)].item()  # Probability of the next character\n","            if prob > 0:\n","                total_loss += -np.log(prob)  # Negative log-likelihood\n","            total_chars += 1\n","\n","    return total_loss / total_chars  # Average loss per character\n"],"metadata":{"id":"D3jT3y0EIKtC","executionInfo":{"status":"ok","timestamp":1736277681267,"user_tz":300,"elapsed":1,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":152,"outputs":[]},{"cell_type":"code","source":["alphas = np.arange(0.0,10,0.5)\n","\n","train_losses = []\n","dev_losses = []\n","\n","for alpha in alphas:\n","    smoothed_trigram_probs = compute_trigram_probs_with_smoothing(trigram_counts, alpha)\n","\n","    train_loss = compute_loss(train_words, smoothed_trigram_probs, 3)\n","    dev_loss = compute_loss(dev_words, smoothed_trigram_probs, 3)\n","\n","    train_losses.append(train_loss)\n","    dev_losses.append(dev_loss)\n","\n","    print(f\"Alpha: {alpha:.1f} | Train Loss: {train_loss:.4f}, Dev Loss: {dev_loss:.4f}\")\n","\n","best_alpha_idx = np.argmin(dev_losses)\n","best_alpha = alphas[best_alpha_idx]\n","\n","best_trigram_probs = compute_trigram_probs_with_smoothing(trigram_counts, best_alpha)\n","test_loss = compute_loss(test_words, best_trigram_probs, 3)\n","\n","print(f\"\\nBest Alpha: {best_alpha:.1f}\")\n","print(f\"Test Loss with Best Alpha: {test_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1LWONDKVINDU","executionInfo":{"status":"ok","timestamp":1736277725628,"user_tz":300,"elapsed":44362,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"86043aed-34db-4e6d-ef34-935d7378ee9b"},"execution_count":153,"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha: 0.0 | Train Loss: 2.0567, Dev Loss: 2.0555\n","Alpha: 0.5 | Train Loss: 2.0785, Dev Loss: 2.1188\n","Alpha: 1.0 | Train Loss: 2.0948, Dev Loss: 2.1297\n","Alpha: 1.5 | Train Loss: 2.1091, Dev Loss: 2.1406\n","Alpha: 2.0 | Train Loss: 2.1219, Dev Loss: 2.1512\n","Alpha: 2.5 | Train Loss: 2.1338, Dev Loss: 2.1612\n","Alpha: 3.0 | Train Loss: 2.1449, Dev Loss: 2.1709\n","Alpha: 3.5 | Train Loss: 2.1554, Dev Loss: 2.1801\n","Alpha: 4.0 | Train Loss: 2.1653, Dev Loss: 2.1889\n","Alpha: 4.5 | Train Loss: 2.1748, Dev Loss: 2.1974\n","Alpha: 5.0 | Train Loss: 2.1838, Dev Loss: 2.2056\n","Alpha: 5.5 | Train Loss: 2.1925, Dev Loss: 2.2136\n","Alpha: 6.0 | Train Loss: 2.2008, Dev Loss: 2.2212\n","Alpha: 6.5 | Train Loss: 2.2089, Dev Loss: 2.2287\n","Alpha: 7.0 | Train Loss: 2.2166, Dev Loss: 2.2359\n","Alpha: 7.5 | Train Loss: 2.2242, Dev Loss: 2.2429\n","Alpha: 8.0 | Train Loss: 2.2315, Dev Loss: 2.2497\n","Alpha: 8.5 | Train Loss: 2.2386, Dev Loss: 2.2564\n","Alpha: 9.0 | Train Loss: 2.2454, Dev Loss: 2.2629\n","Alpha: 9.5 | Train Loss: 2.2521, Dev Loss: 2.2692\n","\n","Best Alpha: 0.0\n","Test Loss with Best Alpha: 2.0567\n"]}]},{"cell_type":"code","source":["# Tried many alphas, any smoothing makes loss worse"],"metadata":{"id":"piDkfLCWKE75","executionInfo":{"status":"ok","timestamp":1736277725628,"user_tz":300,"elapsed":3,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":["E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n"],"metadata":{"id":"OEshWhAJK8XQ"}},{"cell_type":"code","source":["xs, ys = [], []\n","for w in words:\n","    chs = ['.'] + list(w) + ['.']\n","    for ch1, ch2 in zip(chs, chs[1:]):\n","        ix1 = stoi[ch1]\n","        ix2 = stoi[ch2]\n","        xs.append(ix1)\n","        ys.append(ix2)\n","xs = torch.tensor(xs)\n","ys = torch.tensor(ys)\n","num = xs.nelement()\n","print('number of examples: ', num)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ib8oq4H6La4X","executionInfo":{"status":"ok","timestamp":1736277725858,"user_tz":300,"elapsed":232,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"a0007ebe-0fe3-4aa1-ccc2-2ba5129c7a8e"},"execution_count":155,"outputs":[{"output_type":"stream","name":"stdout","text":["number of examples:  228146\n"]}]},{"cell_type":"code","source":["g = torch.Generator()\n","W = torch.randn((27, 27), generator=g, requires_grad=True)\n"],"metadata":{"id":"Ouvt6bJULdj9","executionInfo":{"status":"ok","timestamp":1736278130496,"user_tz":300,"elapsed":102,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":180,"outputs":[]},{"cell_type":"code","source":["# Gradient descent\n","for k in range(500):\n","    logits = W[xs]\n","    counts = logits.exp()\n","    probs = counts / counts.sum(1, keepdims=True)\n","    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n","\n","    W.grad = None\n","    loss.backward()\n","\n","    W.data += -50 * W.grad\n","print(loss.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qiRKM54NLfxp","executionInfo":{"status":"ok","timestamp":1736278196971,"user_tz":300,"elapsed":65419,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"c2acfade-ed3c-4d7b-d8b9-7edf28600078"},"execution_count":181,"outputs":[{"output_type":"stream","name":"stdout","text":["2.4807074069976807\n"]}]},{"cell_type":"code","source":["# Sampling\n","g = torch.Generator()\n","\n","for i in range(50):\n","    out = []\n","    ix = 0\n","    while True:\n","        logits = W[ix]  # Directly access the row of W for the current index\n","        counts = logits.exp()\n","        probs = counts / counts.sum()\n","        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n","        out.append(itos[ix])\n","        if ix == 0:\n","            break\n","    print(''.join(out))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbPf5nJpLu_E","executionInfo":{"status":"ok","timestamp":1736278223789,"user_tz":300,"elapsed":105,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"0848f7a0-ce6f-49d5-9ff7-eb96de9eb071"},"execution_count":184,"outputs":[{"output_type":"stream","name":"stdout","text":["ce.\n","iatelfigghyn.\n","yla.\n","tuliaayerttierarie.\n","maieyn.\n","stanira.\n","klon.\n","mi.\n","bl.\n","khialolerar.\n","zananibrig.\n","bleianey.\n","e.\n","vaamale.\n","morina.\n","igh.\n","a.\n","bra.\n","ca.\n","ahanaroushie.\n","rananusengua.\n","lonaniyl.\n","reren.\n","sxfosufoh.\n","mmyssonn.\n","safelya.\n","eri.\n","giaereksaayrilaia.\n","dilieo.\n","k.\n","ckiykagammynisha.\n","kemad.\n","coc.\n","ne.\n","kl.\n","jade.\n","kayaman.\n","n.\n","tolelosmay.\n","katten.\n",".\n","kalellirsiamar.\n","mardy.\n","a.\n","cevoleusann.\n","diestelli.\n","t.\n","dishlinatararobariyan.\n","e.\n","jaileshalamh.\n"]}]},{"cell_type":"markdown","source":["E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?\n"],"metadata":{"id":"ze2oIHKXP9z-"}},{"cell_type":"code","source":["\n","xs, ys = [], []\n","for w in words:\n","    chs = ['.'] + list(w) + ['.']\n","    for ch1, ch2 in zip(chs, chs[1:]):\n","        ix1 = stoi[ch1]\n","        ix2 = stoi[ch2]\n","        xs.append(ix1)\n","        ys.append(ix2)\n","xs = torch.tensor(xs)\n","ys = torch.tensor(ys)\n","num = xs.nelement()\n","\n","g = torch.Generator().manual_seed(99)\n","W = torch.randn((27, 27), generator=g, requires_grad=True)\n"],"metadata":{"id":"c5hTTdXXP-aw","executionInfo":{"status":"ok","timestamp":1736278541118,"user_tz":300,"elapsed":350,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":195,"outputs":[]},{"cell_type":"code","source":["\n","for k in range(50):\n","    logits = W[xs]\n","    loss = F.cross_entropy(logits, ys) + 0.01 * (W**2).mean()\n","\n","    W.grad = None\n","    loss.backward()\n","\n","    W.data += -50 * W.grad\n","\n","print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pljLrQaKQgrc","executionInfo":{"status":"ok","timestamp":1736278547949,"user_tz":300,"elapsed":5628,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"14874b0b-b7c7-42f0-ff41-f0e3a57a17bb"},"execution_count":196,"outputs":[{"output_type":"stream","name":"stdout","text":["2.5087413787841797\n"]}]},{"cell_type":"markdown","source":["E06: meta-exercise! Think of a fun/interesting exercise and complete it.\n"],"metadata":{"id":"KJFtmy9qR863"}},{"cell_type":"code","source":["# Idea: Fixed characters for start/stop, that is, given i, j, generates names of the form \"i-----j\"\n","# Issue: This turned out to generate nonsense that was too long, any length contrainsts also sucked\n","# Solution - added a bias \\alpha towards the stop character\n","# \\alpha is arbitrary, and depends too heavily on the particular start/stop, maybe fix later"],"metadata":{"id":"3yrtLYGdR-g8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#First, with bigram counts, then with a NN\n","\n","def interpolate_name(model_probs, start_char, end_char, max_len=20, alpha=0.5):\n","    start_idx = stoi[start_char]\n","    end_idx = stoi[end_char]\n","    current_idx = start_idx\n","    name = [start_char]\n","    steps = 0\n","\n","    while steps < max_len and current_idx != end_idx:\n","        probs = model_probs[current_idx].float()\n","\n","        # Zero out probabilities for start/stop sequence\n","        probs[stoi['.']] = 0\n","        probs = probs / probs.sum()\n","\n","        # Interpolate toward the end character\n","        bias = torch.zeros_like(probs)\n","        bias[end_idx] = 1.0  # Full bias toward the end character\n","        probs = (1 - alpha) * probs + alpha * bias  # Weighted combination\n","        probs = probs / probs.sum()\n","\n","        current_idx = torch.multinomial(probs, num_samples=1).item()\n","        name.append(itos[current_idx])\n","\n","        steps += 1\n","\n","    return ''.join(name)\n"],"metadata":{"id":"-OPdwGORSU-r","executionInfo":{"status":"ok","timestamp":1736279451906,"user_tz":300,"elapsed":101,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":216,"outputs":[]},{"cell_type":"code","source":["# Train the bigram model\n","bigram_counts = torch.zeros((27, 27), dtype=torch.float32)\n","for word in words:\n","    chs = ['.'] + list(word) + ['.']\n","    for ch1, ch2 in zip(chs, chs[1:]):\n","        ix1 = stoi[ch1]\n","        ix2 = stoi[ch2]\n","        bigram_counts[ix1, ix2] += 1\n","\n","bigram_probs = bigram_counts / bigram_counts.sum(dim=1, keepdim=True)\n"],"metadata":{"id":"ty44MNTPSe2u","executionInfo":{"status":"ok","timestamp":1736279459285,"user_tz":300,"elapsed":4074,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":217,"outputs":[]},{"cell_type":"code","source":["# Generate names starting from 'a' and stopping at 'h'\n","start_char = 'a'\n","end_char = 'h'\n","for i in range(10):\n","    alpha = 0.15  # Hand chosen :/\n","    name = interpolate_name(bigram_probs, start_char, end_char, alpha=alpha)\n","    print(f\"{name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEL3cUzjSgSs","executionInfo":{"status":"ok","timestamp":1736279560286,"user_tz":300,"elapsed":88,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"bf077279-5941-495f-9583-a63e50a340cc"},"execution_count":231,"outputs":[{"output_type":"stream","name":"stdout","text":["anstih\n","aleth\n","aliirawanah\n","ah\n","aieladolh\n","atlesh\n","alynih\n","anysadenvalanih\n","assomiaderh\n","ah\n"]}]},{"cell_type":"code","source":["## Neural Net approach\n","\n","def interpolate_name_nn(W, start_char, end_char, max_len=20, alpha=0.5):\n","\n","    start_idx = stoi[start_char]\n","    end_idx = stoi[end_char]\n","    current_idx = start_idx\n","    name = [start_char]\n","    steps = 0\n","\n","    while steps < max_len and current_idx != end_idx:\n","        logits = W[current_idx].clone()\n","\n","        logits[stoi['.']] = float('-inf')\n","        logits = logits - logits.max()\n","\n","        probs = logits.exp()\n","        probs = probs / probs.sum()\n","\n","        bias = torch.zeros_like(probs)\n","        bias[end_idx] = 1.0\n","        probs = (1 - alpha) * probs + alpha * bias\n","        probs = probs / probs.sum()\n","\n","        current_idx = torch.multinomial(probs, num_samples=1).item()\n","        name.append(itos[current_idx])\n","\n","        steps += 1\n","\n","    return ''.join(name)\n","\n","xs, ys = [], []\n","for w in words:\n","    chs = ['.'] + list(w) + ['.']\n","    for ch1, ch2 in zip(chs, chs[1:]):\n","        ix1 = stoi[ch1]\n","        ix2 = stoi[ch2]\n","        xs.append(ix1)\n","        ys.append(ix2)\n","xs = torch.tensor(xs)\n","ys = torch.tensor(ys)\n","num = xs.nelement()\n","print('number of examples: ', num)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0nU4bZ2RVMrw","executionInfo":{"status":"ok","timestamp":1736279912195,"user_tz":300,"elapsed":386,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"39b39e10-77f6-4d5a-bab7-d5fe48af8f24"},"execution_count":249,"outputs":[{"output_type":"stream","name":"stdout","text":["number of examples:  228146\n"]}]},{"cell_type":"code","source":["g = torch.Generator()\n","W = torch.randn((27, 27), generator=g, requires_grad=True)"],"metadata":{"id":"2woFnKZ4VX6M","executionInfo":{"status":"ok","timestamp":1736279914862,"user_tz":300,"elapsed":110,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":250,"outputs":[]},{"cell_type":"code","source":["for k in range(50):\n","    logits = W[xs]\n","    loss = F.cross_entropy(logits, ys) + 0.01 * (W**2).mean()\n","    W.grad = None\n","    loss.backward()\n","    W.data += -50 * W.grad\n","print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ME4Yd_6jVcY7","executionInfo":{"status":"ok","timestamp":1736279920780,"user_tz":300,"elapsed":5020,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"27d6d593-ee39-4af1-e909-1b1a74266b68"},"execution_count":251,"outputs":[{"output_type":"stream","name":"stdout","text":["2.5101189613342285\n"]}]},{"cell_type":"code","source":["# Generate names between 'a' and 'h'\n","start_char = 'a'\n","end_char = 'h'\n","for i in range(10):\n","    alpha = 0.2  # Random interpolation strength\n","    name = interpolate_name_nn(W, start_char, end_char, alpha=alpha)\n","    print(f\"{name}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K90dr_2oVl2q","executionInfo":{"status":"ok","timestamp":1736279970720,"user_tz":300,"elapsed":177,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"4b63fcb1-b981-4a4b-f649-bbefe5233a50"},"execution_count":255,"outputs":[{"output_type":"stream","name":"stdout","text":["anndh\n","avynsantcarh\n","airah\n","angineleh\n","ah\n","aynalecenadeh\n","alleleynnh\n","ah\n","annah\n","ananapbllalh\n"]}]}]}