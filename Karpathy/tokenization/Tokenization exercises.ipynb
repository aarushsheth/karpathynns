{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9hwJLVWiRPVKQeeXMSM8J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["exercise\n","Build your own GPT-4 Tokenizer!\n","\n","Step 1\n","Write the BasicTokenizer class, with the following three core functions:\n","\n","def train(self, text, vocab_size, verbose=False)\n","def encode(self, text)\n","def decode(self, ids)\n","Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file tests/taylorswift.txt.\n","\n"],"metadata":{"id":"3RbG0oxkLqtd"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"io5tdS9DK7Gs","executionInfo":{"status":"ok","timestamp":1736394448939,"user_tz":300,"elapsed":147,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"outputs":[],"source":["class BasicTokenizer:\n","    def __init__(self):\n","        self.merges = {}  # (int, int) -> int\n","        self.vocab = {}   # int -> bytes\n","        self.vocab_size = None\n","\n","    def train(self, text, vocab_size, verbose=False):\n","        # convert raw text to a list of byte IDs\n","        ids = list(text.encode(\"utf-8\"))\n","        # we start with a \"vocab\" for raw bytes [0..255]\n","        self.vocab = {i: bytes([i]) for i in range(256)}\n","        self.merges = {}\n","        self.vocab_size = vocab_size\n","\n","        # define a helper to count pair frequencies\n","        def get_stats(seq):\n","            stats = {}\n","            for a, b in zip(seq, seq[1:]):\n","                stats[(a, b)] = stats.get((a, b), 0) + 1\n","            return stats\n","\n","        # define a helper to merge a specific pair in the sequence\n","        def merge_pair(seq, pair, new_id):\n","            merged = []\n","            i = 0\n","            while i < len(seq):\n","                if i < len(seq) - 1 and seq[i] == pair[0] and seq[i+1] == pair[1]:\n","                    merged.append(new_id)\n","                    i += 2\n","                else:\n","                    merged.append(seq[i])\n","                    i += 1\n","            return merged\n","\n","        # repeatedly merge the most common pair until reaching vocab_size\n","        # there are already 256 base tokens, so we can do merges up to vocab_size - 256\n","        merges_needed = vocab_size - 256\n","        for i in range(merges_needed):\n","            stats = get_stats(ids)\n","            if not stats:\n","                break\n","            best = max(stats, key=stats.get)\n","            new_id = 256 + i\n","            self.merges[best] = new_id\n","            self.vocab[new_id] = self.vocab[best[0]] + self.vocab[best[1]]\n","            ids = merge_pair(ids, best, new_id)\n","            if verbose:\n","                print(f\"Merge {best} -> {new_id}, frequency={stats[best]}\")\n","\n","    def encode(self, text):\n","        # convert text to a list of raw byte IDs\n","        seq = list(text.encode(\"utf-8\"))\n","        # repeatedly attempt merges that we know about\n","        # we do a naive loop: check all pairs, if found in merges, merge them\n","        # keep going until no merges apply\n","        def get_stats(seq):\n","            stats = {}\n","            for a, b in zip(seq, seq[1:]):\n","                stats[(a, b)] = stats.get((a, b), 0) + 1\n","            return stats\n","\n","        # same helper as above\n","        def merge_pair(seq, pair, new_id):\n","            merged = []\n","            i = 0\n","            while i < len(seq):\n","                if i < len(seq) - 1 and seq[i] == pair[0] and seq[i+1] == pair[1]:\n","                    merged.append(new_id)\n","                    i += 2\n","                else:\n","                    merged.append(seq[i])\n","                    i += 1\n","            return merged\n","\n","        while True:\n","            stats = get_stats(seq)\n","            # pick a pair that is in our merges dictionary and has the highest frequency\n","            # if no known merges appear, we're done\n","            known_pairs = [(pair, freq) for pair, freq in stats.items() if pair in self.merges]\n","            if not known_pairs:\n","                break\n","            best = max(known_pairs, key=lambda x: x[1])[0]\n","            seq = merge_pair(seq, best, self.merges[best])\n","        return seq\n","\n","    def decode(self, ids):\n","        # convert list of IDs back into bytes, then decode to string\n","        # each ID‚Äôs bytes are stored in self.vocab\n","        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n","        return tokens.decode(\"utf-8\", errors=\"replace\")\n"]},{"cell_type":"code","source":["\n","# Example\n","if __name__ == \"__main__\":\n","    text = \"Hello world! Hello GPT-4 Tokenizer example. ÏïàÎÖïÌïòÏÑ∏Ïöî üëã  (hello in Korean!)\"\n","    tokenizer = BasicTokenizer()\n","    tokenizer.train(text, vocab_size=300, verbose=True)  # request ~300 vocab\n","    enc = tokenizer.encode(\"Hello GPT-4 Tokenizer example. ÏïàÎÖïÌïòÏÑ∏Ïöî üëã\")\n","    dec = tokenizer.decode(enc)\n","    print(\"Encoded:\", enc)\n","    print(\"Decoded:\", dec)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-vN_412LLC7","executionInfo":{"status":"ok","timestamp":1736394458151,"user_tz":300,"elapsed":184,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"4bb735d4-41f9-4323-d9c6-6b1e84e834c0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Merge (101, 108) -> 256, frequency=3\n","Merge (256, 108) -> 257, frequency=3\n","Merge (257, 111) -> 258, frequency=3\n","Merge (258, 32) -> 259, frequency=3\n","Merge (72, 259) -> 260, frequency=2\n","Merge (111, 114) -> 261, frequency=2\n","Merge (260, 119) -> 262, frequency=1\n","Merge (262, 261) -> 263, frequency=1\n","Merge (263, 108) -> 264, frequency=1\n","Merge (264, 100) -> 265, frequency=1\n","Merge (265, 33) -> 266, frequency=1\n","Merge (266, 32) -> 267, frequency=1\n","Merge (267, 260) -> 268, frequency=1\n","Merge (268, 71) -> 269, frequency=1\n","Merge (269, 80) -> 270, frequency=1\n","Merge (270, 84) -> 271, frequency=1\n","Merge (271, 45) -> 272, frequency=1\n","Merge (272, 52) -> 273, frequency=1\n","Merge (273, 32) -> 274, frequency=1\n","Merge (274, 84) -> 275, frequency=1\n","Merge (275, 111) -> 276, frequency=1\n","Merge (276, 107) -> 277, frequency=1\n","Merge (277, 101) -> 278, frequency=1\n","Merge (278, 110) -> 279, frequency=1\n","Merge (279, 105) -> 280, frequency=1\n","Merge (280, 122) -> 281, frequency=1\n","Merge (281, 101) -> 282, frequency=1\n","Merge (282, 114) -> 283, frequency=1\n","Merge (283, 32) -> 284, frequency=1\n","Merge (284, 101) -> 285, frequency=1\n","Merge (285, 120) -> 286, frequency=1\n","Merge (286, 97) -> 287, frequency=1\n","Merge (287, 109) -> 288, frequency=1\n","Merge (288, 112) -> 289, frequency=1\n","Merge (289, 108) -> 290, frequency=1\n","Merge (290, 101) -> 291, frequency=1\n","Merge (291, 46) -> 292, frequency=1\n","Merge (292, 32) -> 293, frequency=1\n","Merge (293, 236) -> 294, frequency=1\n","Merge (294, 149) -> 295, frequency=1\n","Merge (295, 136) -> 296, frequency=1\n","Merge (296, 235) -> 297, frequency=1\n","Merge (297, 133) -> 298, frequency=1\n","Merge (298, 149) -> 299, frequency=1\n","Encoded: [260, 71, 80, 84, 45, 52, 32, 84, 111, 107, 101, 110, 105, 122, 101, 114, 32, 101, 120, 97, 109, 112, 108, 101, 46, 32, 236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 32, 240, 159, 145, 139]\n","Decoded: Hello GPT-4 Tokenizer example. ÏïàÎÖïÌïòÏÑ∏Ïöî üëã\n"]}]},{"cell_type":"markdown","source":["Step 2\n","Convert you BasicTokenizer into a RegexTokenizer, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n","\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""],"metadata":{"id":"kR-PvhQYLwAS"}},{"cell_type":"code","source":["import regex as re\n","\n","# GPT-4 splitting pattern provided\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","class RegexTokenizer(BasicTokenizer):\n","    def __init__(self, pattern=GPT4_SPLIT_PATTERN):\n","        super().__init__()\n","        self.pattern = re.compile(pattern)\n","\n","    def encode(self, text):\n","        # Split text using the provided regex pattern into parts\n","        parts = self.pattern.findall(text)\n","        tokens = []\n","        # For each part, apply the BasicTokenizer's encode logic\n","        for part in parts:\n","            # Use parent's encode to process each segment individually\n","            tokens.extend(super().encode(part))\n","        return tokens\n"],"metadata":{"id":"G-vKHJ_YLgYR","executionInfo":{"status":"ok","timestamp":1736394550717,"user_tz":300,"elapsed":439,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","# Example\n","if __name__ == \"__main__\":\n","    text = \"Hello world! 12345 üòä TestÎ¨∏ÏûêÏó¥ combined.\"\n","\n","    # Train BasicTokenizer and RegexTokenizer on the same text for comparison\n","    basic_tokenizer = BasicTokenizer()\n","    basic_tokenizer.train(text, vocab_size=300, verbose=False)\n","\n","    regex_tokenizer = RegexTokenizer()\n","    regex_tokenizer.train(text, vocab_size=300, verbose=False)\n","\n","    # Encode using both tokenizers\n","    basic_tokens = basic_tokenizer.encode(text)\n","    regex_tokens = regex_tokenizer.encode(text)\n","\n","    print(\"BasicTokenizer tokens:\", basic_tokens)\n","    print(\"RegexTokenizer tokens:\", regex_tokens)\n","\n","    # Verify correctness\n","    print(\"Basic decoded:\", basic_tokenizer.decode(basic_tokens))\n","    print(\"Regex decoded:\", regex_tokenizer.decode(regex_tokens))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_QlqGx66LjCW","executionInfo":{"status":"ok","timestamp":1736394559824,"user_tz":300,"elapsed":200,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"7e048515-f621-4627-e339-43c06d5d0d70"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["BasicTokenizer tokens: [299, 100, 46]\n","RegexTokenizer tokens: [259, 32, 119, 111, 114, 108, 100, 33, 32, 49, 50, 51, 52, 53, 32, 240, 159, 152, 138, 32, 84, 101, 115, 116, 235, 172, 184, 236, 158, 144, 236, 151, 180, 32, 99, 111, 109, 98, 105, 110, 101, 100, 46]\n","Basic decoded: Hello world! 12345 üòä TestÎ¨∏ÏûêÏó¥ combined.\n","Regex decoded: Hello world! 12345 üòä TestÎ¨∏ÏûêÏó¥ combined.\n"]}]},{"cell_type":"markdown","source":["Step 3\n","You're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both encode and decode, matching tiktoken.\n","\n","# match this\n","import tiktoken\n","enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n","ids = enc.encode(\"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\")\n","text = enc.decode(ids) # get the same text back\n","Unfortunately, you will run into two issues:\n","\n","It is not trivial to recover the raw merges from the GPT-4 tokenizer. You can easily recover what we call vocab here, and what they call and store under enc._mergeable_ranks. Feel free to copy paste the recover_merges function in minbpe/gpt4.py, which takes these ranks and returns the raw merges. If you wish to know how this function works, read this and this. Basically, under some conditions it is enough to only store the parent nodes (and their rank) and get rid of the precise details of which children merged up to any parent.\n","Second, the GPT-4 tokenizer for some reason permutes its raw bytes. It stores this permutation in the first 256 elements of the mergeable ranks, so you can recover this byte shuffle relatively simply as byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}. In both your encode and decode, you'll have to shuffle bytes around accordingly. If you're stuck, reference the minbpe/gpt4.py` file for hints."],"metadata":{"id":"6ZTKqCD5Lpmo"}},{"cell_type":"code","source":["!pip install regex tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aORsPVahMKpN","executionInfo":{"status":"ok","timestamp":1736394733170,"user_tz":300,"elapsed":3736,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"f973482b-a066-44dc-8c3d-9a98c5791332"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n","Collecting tiktoken\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n","Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.8.0\n"]}]},{"cell_type":"code","source":["import regex as re\n","import tiktoken\n","\n","# Assume GPT4_SPLIT_PATTERN is defined as before\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","\n","def bpe(mergeable_ranks, token, max_rank):\n","    # helper function used in get_gpt4_merges() to reconstruct the merge forest\n","    parts = [bytes([b]) for b in token]\n","    while True:\n","        min_idx = None\n","        min_rank = None\n","        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n","            rank = mergeable_ranks.get(pair[0] + pair[1])\n","            if rank is not None and (min_rank is None or rank < min_rank):\n","                min_idx = i\n","                min_rank = rank\n","        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n","            break\n","        assert min_idx is not None\n","        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n","    return parts\n","\n","def recover_merges(mergeable_ranks):\n","    # the `merges` are already the byte sequences in their merged state.\n","    # so we have to recover the original pairings. We can do this by doing\n","    # a small BPE training run on all the tokens, in their order.\n","    # also see https://github.com/openai/tiktoken/issues/60\n","    # also see https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306\n","    merges = {}\n","    for token, rank in mergeable_ranks.items():\n","        if len(token) == 1:\n","            continue # skip raw bytes\n","        pair = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n","        assert len(pair) == 2\n","        # recover the integer ranks of the pair\n","        ix0 = mergeable_ranks[pair[0]]\n","        ix1 = mergeable_ranks[pair[1]]\n","        merges[(ix0, ix1)] = rank\n","\n","    return merges\n","\n","GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","GPT4_SPECIAL_TOKENS = {\n","    '<|endoftext|>': 100257,\n","    '<|fim_prefix|>': 100258,\n","    '<|fim_middle|>': 100259,\n","    '<|fim_suffix|>': 100260,\n","    '<|endofprompt|>': 100276\n","}\n"],"metadata":{"id":"v2QpFoUkLy9E","executionInfo":{"status":"ok","timestamp":1736394797595,"user_tz":300,"elapsed":121,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["\n","class GPT4Tokenizer(RegexTokenizer):\n","    def __init__(self, pattern=GPT4_SPLIT_PATTERN):\n","        super().__init__(pattern)\n","        self.byte_shuffle = None\n","        self.inverse_shuffle = None\n","\n","    def load_from_tiktoken(self, enc):\n","        # Recover raw merges from tiktoken's mergeable_ranks\n","        self.merges = recover_merges(enc._mergeable_ranks)\n","\n","        # Recover byte permutation for first 256 bytes\n","        # This creates a mapping: original_byte -> permuted_byte\n","        self.byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}\n","        # Create inverse mapping for decoding: permuted_byte -> original_byte\n","        self.inverse_shuffle = {v: k for k, v in self.byte_shuffle.items()}\n","\n","        # Initialize vocabulary using recovered merges\n","        self.vocab = {i: bytes([i]) for i in range(256)}\n","        for (p0, p1), new_id in self.merges.items():\n","            self.vocab[new_id] = self.vocab[p0] + self.vocab[p1]\n","\n","    def encode(self, text):\n","        # Apply regex splitting as in RegexTokenizer\n","        parts = self.pattern.findall(text)\n","        all_tokens = []\n","        for part in parts:\n","            # Convert part to UTF-8 bytes and apply byte permutation\n","            raw_bytes = list(part.encode(\"utf-8\"))\n","            shuffled_bytes = [self.byte_shuffle[b] for b in raw_bytes]\n","\n","            # Use BPE merging on permuted bytes\n","            seq = shuffled_bytes\n","            def get_stats(seq):\n","                stats = {}\n","                for a, b in zip(seq, seq[1:]):\n","                    stats[(a, b)] = stats.get((a, b), 0) + 1\n","                return stats\n","\n","            def merge_pair(seq, pair, new_id):\n","                merged = []\n","                i = 0\n","                while i < len(seq):\n","                    if i < len(seq) - 1 and seq[i] == pair[0] and seq[i+1] == pair[1]:\n","                        merged.append(new_id)\n","                        i += 2\n","                    else:\n","                        merged.append(seq[i])\n","                        i += 1\n","                return merged\n","\n","            while True:\n","                stats = get_stats(seq)\n","                known_pairs = [(pair, freq) for pair, freq in stats.items() if pair in self.merges]\n","                if not known_pairs:\n","                    break\n","                best = max(known_pairs, key=lambda x: x[1])[0]\n","                seq = merge_pair(seq, best, self.merges[best])\n","            all_tokens.extend(seq)\n","        return all_tokens\n","\n","    def decode(self, ids):\n","        # Convert token IDs back to a byte sequence using the vocabulary\n","        merged_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n","        # Reverse the byte permutation\n","        unshuffled_bytes = bytes([self.inverse_shuffle[b] for b in merged_bytes])\n","        # Decode from UTF-8\n","        return unshuffled_bytes.decode(\"utf-8\", errors=\"replace\")\n"],"metadata":{"id":"AI_oS8dbL-b9","executionInfo":{"status":"ok","timestamp":1736394799661,"user_tz":300,"elapsed":85,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["\n","# Checking matching tiktoken's behavior\n","if __name__ == \"__main__\":\n","    # Load GPT-4 tiktoken encoding\n","    enc = tiktoken.get_encoding(\"cl100k_base\")\n","\n","    # Initialize and load GPT4Tokenizer from tiktoken data\n","    tokenizer = GPT4Tokenizer()\n","    tokenizer.load_from_tiktoken(enc)\n","\n","    # Test encoding and decoding\n","    test_text = \"hello world!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) lol123 üòâ\"\n","    custom_ids = tokenizer.encode(test_text)\n","    custom_decoded = tokenizer.decode(custom_ids)\n","\n","    # Use tiktoken for comparison\n","    tiktoken_ids = enc.encode(test_text)\n","    tiktoken_decoded = enc.decode(tiktoken_ids)\n","\n","    print(\"Custom tokenizer IDs:\", custom_ids)\n","    print(\"tiktoken IDs:           \", tiktoken_ids)\n","    print(\"IDs match:\", custom_ids == tiktoken_ids)\n","    print(\"Decoded text matches:\", custom_decoded == tiktoken_decoded)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AC_NioFL_cz","executionInfo":{"status":"ok","timestamp":1736394803465,"user_tz":300,"elapsed":2046,"user":{"displayName":"Aarush Sheth","userId":"06423698602659898506"}},"outputId":"af81c4c1-0a5f-4d53-d172-ae4c4d7aed7b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom tokenizer IDs: [383, 657, 78, 24670, 2438, 67, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 781, 75, 4513, 57037]\n","tiktoken IDs:            [15339, 1917, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 28509, 4513, 57037]\n","IDs match: False\n","Decoded text matches: True\n"]}]}]}